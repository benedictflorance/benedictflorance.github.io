<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>NLP | Benedict Florance Arockiaraj</title>
    <link>https://benedictflorance.github.io/tag/nlp/</link>
      <atom:link href="https://benedictflorance.github.io/tag/nlp/index.xml" rel="self" type="application/rss+xml" />
    <description>NLP</description>
    <generator>Wowchemy (https://wowchemy.com)</generator><language>en-us</language><lastBuildDate>Mon, 15 May 2023 00:00:00 +0000</lastBuildDate>
    <image>
      <url>https://benedictflorance.github.io/media/icon_hu9eeb12546e8517e7f9c186d851ae3ad9_1969_512x512_fill_lanczos_center_3.png</url>
      <title>NLP</title>
      <link>https://benedictflorance.github.io/tag/nlp/</link>
    </image>
    
    <item>
      <title>Linguistic Properties of Truthful Responses</title>
      <link>https://benedictflorance.github.io/research/acl/</link>
      <pubDate>Mon, 15 May 2023 00:00:00 +0000</pubDate>
      <guid>https://benedictflorance.github.io/research/acl/</guid>
      <description>&lt;ul&gt;
&lt;li&gt;We investigate the phenomenon of an LLM&amp;rsquo;s untruthful response using a large set of 220 handcrafted linguistic features. We focus on GPT-3 models and find that the linguistic profiles of responses are similar across model sizes.&lt;/li&gt;
&lt;li&gt;That is, how varying-sized LLMs respond to given prompts stays similar on the linguistic properties level.&lt;/li&gt;
&lt;li&gt;We expand upon this finding by training support vector machines that rely only upon the stylistic components of model responses to classify the truthfulness of statements.&lt;/li&gt;
&lt;li&gt;Though the dataset size limits our current findings, we present promising evidence that truthfulness detection is possible without evaluating the content itself.&lt;/li&gt;
&lt;li&gt;This paper was accepted at TrustNLP @ ACL 2023&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Sensible Universal Adversarial Triggers</title>
      <link>https://benedictflorance.github.io/research/adversarial_triggers/</link>
      <pubDate>Mon, 27 Dec 2021 00:00:00 +0000</pubDate>
      <guid>https://benedictflorance.github.io/research/adversarial_triggers/</guid>
      <description>&lt;ul&gt;
&lt;li&gt;Proposed a novel technique combining parts-of-speech filtering and perplexity based loss
function to generate sensible triggers that are
closer to natural phrases.&lt;/li&gt;
&lt;li&gt;For the task of sentiment analysis on the SST dataset, the method
produced sensible triggers that achieve accura-
cies as low as 4% and 12% for flipping positive to negative predictions and vice-versa.&lt;/li&gt;
&lt;li&gt;To
build robust models, performed adversarial training using the generated triggers that increases the accuracy of the model from 12% to
48%.&lt;/li&gt;
&lt;li&gt;Illustrated that adversarial at-
tacks can be made difficult to detect by generating sensible triggers, and to facilitate robust
model development through relevant defenses.&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
  </channel>
</rss>
