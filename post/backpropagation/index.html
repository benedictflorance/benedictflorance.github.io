<!DOCTYPE html>
<html lang="en-us">
<head>

  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="theme" content="hugo-academic">
  <meta name="generator" content="Hugo 0.42.2" />
  <meta name="author" content="Benedict Florance Arockiaraj">

  
  
  
  
    
      
    
  
  <meta name="description" content="Whether you’re a beginner in machine learning or an expert, you would have had a hard time understanding the concept of backpropagation in neural networks. If you’re a beginner, the first look of the complex steps strung together in the backprop algorithm would’ve been definitely daunting. While a few of us would have spent time in analysing the algorithm and getting the intuition, most of us would have abstracted the learning process.">

  
  <link rel="alternate" hreflang="en-us" href="http://benedictflorance.github.io/post/backpropagation/">

  


  

  
  
  <meta name="theme-color" content="#0095eb">
  
  
  
  
    
  
  
    
    
      
        <link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/github.min.css">
      
    
  
  
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/3.3.7/css/bootstrap.min.css" integrity="sha512-6MXa8B6uaO18Hid6blRMetEIoPqHf7Ux1tnyIQdpt9qI5OACx7C+O3IVTr98vwGnlcg0LOLa02i9Y1HpVhlfiw==" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/academicons/1.8.6/css/academicons.min.css" integrity="sha256-uFVgMKfistnJAfoCUQigIl+JfUaP47GrRKjf6CTPVmw=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css" integrity="sha512-SfTiTlX6kk+qitfevl/7LibUOeJWlt9rbyDn92a1DqWOw9vWG2MFoays0sgObmWazO5BQPiFucnnEAjpAB+/Sw==" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.2.5/jquery.fancybox.min.css" integrity="sha256-ygkqlh3CYSUri3LhQxzdcm0n1EQvH2Y+U5S2idbLtxs=" crossorigin="anonymous">
  
  
  
  
  <link rel="stylesheet" href="//fonts.googleapis.com/css?family=Montserrat:400,700%7cRoboto:400,400italic,700%7cRoboto&#43;Mono">
  
  <link rel="stylesheet" href="/styles.css">
  
  <link rel="stylesheet" href="/css/cv.css">
  

  
  
    <script>
      window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;
      ga('create', 'UA-121989929-1', 'auto');
      
      ga('require', 'eventTracker');
      ga('require', 'outboundLinkTracker');
      ga('require', 'urlChangeTracker');
      ga('send', 'pageview');
    </script>
    <script async src="//www.google-analytics.com/analytics.js"></script>
    
    <script async src="https://cdnjs.cloudflare.com/ajax/libs/autotrack/2.4.1/autotrack.js" integrity="sha512-HUmooslVKj4m6OBu0OgzjXXr+QuFYy/k7eLI5jdeEy/F4RSgMn6XRWRGkFi5IFaFgy7uFTkegp3Z0XnJf3Jq+g==" crossorigin="anonymous"></script>
    
  
  

  
  <link rel="alternate" href="http://benedictflorance.github.io/index.xml" type="application/rss+xml" title="Benedict Florance Arockiaraj">
  <link rel="feed" href="http://benedictflorance.github.io/index.xml" type="application/rss+xml" title="Benedict Florance Arockiaraj">
  

  <link rel="manifest" href="/site.webmanifest">
  <link rel="icon" type="image/x-icon" href="/favicon.ico">
  <link rel="apple-touch-icon" type="image/png" href="/favicon.ico">

  <link rel="canonical" href="http://benedictflorance.github.io/post/backpropagation/">

  <meta property="twitter:card" content="summary_large_image">
  
  <meta property="twitter:site" content="@benedictfloranc">
  <meta property="twitter:creator" content="@benedictfloranc">
  
  <meta property="og:site_name" content="Benedict Florance Arockiaraj">
  <meta property="og:url" content="http://benedictflorance.github.io/post/backpropagation/">
  <meta property="og:title" content="Breaking down Neural Networks: An intuitive approach to backprop! | Benedict Florance Arockiaraj">
  <meta property="og:description" content="Whether you’re a beginner in machine learning or an expert, you would have had a hard time understanding the concept of backpropagation in neural networks. If you’re a beginner, the first look of the complex steps strung together in the backprop algorithm would’ve been definitely daunting. While a few of us would have spent time in analysing the algorithm and getting the intuition, most of us would have abstracted the learning process."><meta property="og:image" content="http://benedictflorance.github.io/img/backpropagation.jpg">
  <meta property="og:locale" content="en-us">
  
  <meta property="article:published_time" content="2018-06-16T23:22:47&#43;05:30">
  
  <meta property="article:modified_time" content="2018-06-16T23:22:47&#43;05:30">
  

  

  

  <title>Breaking down Neural Networks: An intuitive approach to backprop! | Benedict Florance Arockiaraj</title>

</head>
<body id="top" data-spy="scroll" data-target="#toc" data-offset="71" >

<nav class="navbar navbar-default navbar-fixed-top" id="navbar-main">
  <div class="container">

    
    <div class="navbar-header">
      
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse"
              data-target=".navbar-collapse" aria-expanded="false">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      
      <a class="navbar-brand" href="/">Benedict Florance Arockiaraj</a>
    </div>

    
    <div class="collapse navbar-collapse">

      
      
      <ul class="nav navbar-nav navbar-right">
        

        
        
        
        
        
          
        

        <li class="nav-item">
          <a href="/#about">
            
            <span>Home</span>
            
          </a>
        </li>

        
        

        
        
        
        
        
          
        

        <li class="nav-item">
          <a href="/#posts">
            
            <span>Posts</span>
            
          </a>
        </li>

        
        

        
        
        
        
        
          
        

        <li class="nav-item">
          <a href="/#projects">
            
            <span>Projects</span>
            
          </a>
        </li>

        
        

        
        
        
        
        
          
        

        <li class="nav-item">
          <a href="/#resume">
            
            <span>Resume</span>
            
          </a>
        </li>

        
        

        
        
        
        
        
          
        

        <li class="nav-item">
          <a href="/#contact">
            
            <span>Contact</span>
            
          </a>
        </li>

        
        
      

      
      </ul>

    </div>
  </div>
</nav>


<article class="article" itemscope itemtype="http://schema.org/Article">

  
<div class="article-header">
  
  
    <img src="/img/backpropagation.jpg" class="article-banner" itemprop="image">
  

  
</div>



  <div class="article-container">
    <h1 itemprop="name">Breaking down Neural Networks: An intuitive approach to backprop!</h1>

    

<div class="article-metadata">

  
  
  <div itemprop="author">
    Benedict Florance
    
  </div>
  

  <span class="article-date">
    
    <meta content="2018-06-16 23:22:47 &#43;0530 IST" itemprop="datePublished">
    <time datetime="2018-06-16 23:22:47 &#43;0530 IST" itemprop="dateModified">
      Jun 16, 2018
    </time>
  </span>
  <span itemscope itemprop="author publisher" itemtype="http://schema.org/Person">
    <meta itemprop="name" content="Benedict Florance Arockiaraj">
  </span>

  
  <span class="middot-divider"></span>
  <span class="article-reading-time">
    10 min read
  </span>
  

  
  
  <span class="middot-divider"></span>
  <a href="http://benedictflorance.github.io/post/backpropagation/#disqus_thread"></a>
  

  
  
  
  

  
  
<div class="share-box" aria-hidden="true">
  <ul class="share">
    <li>
      <a class="twitter"
         href="https://twitter.com/intent/tweet?text=Breaking%20down%20Neural%20Networks%3a%20An%20intuitive%20approach%20to%20backprop%21&amp;url=http%3a%2f%2fbenedictflorance.github.io%2fpost%2fbackpropagation%2f"
         target="_blank" rel="noopener">
        <i class="fa fa-twitter"></i>
      </a>
    </li>
    <li>
      <a class="facebook"
         href="https://www.facebook.com/sharer.php?u=http%3a%2f%2fbenedictflorance.github.io%2fpost%2fbackpropagation%2f"
         target="_blank" rel="noopener">
        <i class="fa fa-facebook"></i>
      </a>
    </li>
    <li>
      <a class="linkedin"
         href="https://www.linkedin.com/shareArticle?mini=true&amp;url=http%3a%2f%2fbenedictflorance.github.io%2fpost%2fbackpropagation%2f&amp;title=Breaking%20down%20Neural%20Networks%3a%20An%20intuitive%20approach%20to%20backprop%21"
         target="_blank" rel="noopener">
        <i class="fa fa-linkedin"></i>
      </a>
    </li>
    <li>
      <a class="weibo"
         href="http://service.weibo.com/share/share.php?url=http%3a%2f%2fbenedictflorance.github.io%2fpost%2fbackpropagation%2f&amp;title=Breaking%20down%20Neural%20Networks%3a%20An%20intuitive%20approach%20to%20backprop%21"
         target="_blank" rel="noopener">
        <i class="fa fa-weibo"></i>
      </a>
    </li>
    <li>
      <a class="email"
         href="mailto:?subject=Breaking%20down%20Neural%20Networks%3a%20An%20intuitive%20approach%20to%20backprop%21&amp;body=http%3a%2f%2fbenedictflorance.github.io%2fpost%2fbackpropagation%2f">
        <i class="fa fa-envelope"></i>
      </a>
    </li>
  </ul>
</div>


  

</div>


    <div class="article-style" itemprop="articleBody">
      

<p>Whether you’re a beginner in machine learning or an expert, you would have had a hard time understanding the concept of backpropagation in neural networks. If you’re a beginner, the first look of the complex steps strung together in the backprop algorithm would’ve been definitely daunting. While a few of us would have spent time in analysing the algorithm and getting the intuition, most of us would have abstracted the learning process. A common and a very sensible question that can arise for people who have worked with deep learning frameworks like Facebook’s Pytorch and Google’s Tensorflow is:</p>

<blockquote>
<p>&ldquo;Why do we have to spend time in understanding the necessity of backprop if a single line of <code>loss.backward()</code> in Pytorch or <code>tape.gradient(loss_value, model.variables)</code> in TensorFlow can do the magic?&rdquo;</p>
</blockquote>

<p>A one-liner. You can’t debug your neural network effectively and won’t be able to figure out where you are going wrong!</p>

<p>This article aims to explain the intuition behind the backpropagation algorithm in a simple way, that can be understandable even by a machine learning beginner.</p>

<h3 id="a-short-introduction-to-neural-networks">A short introduction to neural networks:</h3>

<p><img src="https://cdn-images-1.medium.com/max/2000/1*WNxN2ArLaGt0-Rm3tzWw1g.jpeg" alt="A neural network with two hidden layers. Source: [www.towardsdatascience.com](http://www.towardsdatascience.com)" /><em>A neural network with two hidden layers. Source: <a href="http://www.towardsdatascience.com" target="_blank">www.towardsdatascience.com</a></em></p>

<blockquote>
<p>The core idea of neural networks is to compute weighted sums of the values in the input layer and create a mapping between the input and output layer by a series of functions (in general, nonlinear functions).</p>
</blockquote>

<p>Sounds complex? Let me break it down. Every neural network has an input layer, a series of hidden layers and an output layer. Let us take the task of classifying our images into four categories ‘cat’, ‘dog’, ‘frog’ and ‘horse’. The values in the <strong>input layer</strong> represent the pixel values of a given image that we want to classify into four categories. The small circles in each layer are called <strong>neurons</strong>. The values of the <strong>output layer</strong> represent the score for each category. We classify the image into the category that gets the highest score. For instance, if the ‘frog’ neuron in the output layer receives the highest value in comparison to the other neurons of the layer, we say the image is a ‘frog’. The other intermediary layers are called <strong>hidden layers.</strong></p>

<p><img src="https://cdn-images-1.medium.com/max/2000/0*4Je3OlrLh-1QIv_V.png" alt="A closer look at a single neuron. Source: Google Images" /><em>A closer look at a single neuron. Source: Google Images</em></p>

<p>Every junction between two layers have a set of parameters called <strong>weights</strong>. These weights are not random numbers, the weight matrix between different layers can be visualized as a template or a feature that we are looking for in the image to classify it. The values of the next layer are calculated by applying a function called <strong>activation function</strong> to the values of the previous layer and the weights in between the two layers. Commonly used activation functions are sigmoid, tanh, Rectified Linear Unit (also called ReLU), leaky ReLU and Maxout. The difference between the output that we predict using the network and the actual class of the image determines the loss of the network. Greater the number of images that we classify correctly, lesser the loss. There are several ways of computing the loss of a neural network. One naive approach would be to find the mean squared error i.e, the mean of squares of the difference between the predicted and actual values. Other techniques that are often used to compute loss are softmax(cross-entropy) and support vector machine (hinge) loss. So, the aim of a neural network problem is to learn the best set of weights to give us the desired scores in the output layer. In other words, to minimize the loss function of the network.</p>

<h2 id="what-is-backpropagation">What is backpropagation?</h2>

<p>Training a neural network typically happens in two phases.</p>

<ol>
<li><p><strong>Forward Pass:</strong> We compute the outputs of every node in the forward pass and calculate the final loss of the network.</p></li>

<li><p><strong>Backward Pass:</strong> We start at the end of the network, backpropagate or feed the errors back, recursively apply chain rule to compute gradients all the way to the inputs of the network and then update the weights. This method of backpropagating the errors and computing the gradients is called <strong>backpropagation.</strong></p></li>
</ol>

<p>It is a very popular neural network training algorithm as it is conceptually clear, computationally tractable and produces optimal results in general. To reiterate, the aim of a typical neural network problem is to discover a model that fits our data ‘best’. Ultimately, we want to minimize the cost or loss function by choosing the best set of parameters.</p>

<h3 id="a-short-recap-of-derivatives">A short recap of derivatives:</h3>

<p>Let us consider the following function.</p>

<p><img src="https://cdn-images-1.medium.com/max/2000/1*haSLzvmimPyGftWBixHxUA.png" alt="Product of two variables." /><em>Product of two variables.</em></p>

<p><img src="https://cdn-images-1.medium.com/max/2000/1*srZ0Vnma4URroyKbeYAGnA.png" alt="Partial derivatives of the function w.r.t the inputs." /><em>Partial derivatives of the function w.r.t the inputs.</em></p>

<p>It is simple enough to find the partial derivative with respect to either of the inputs. For example, if</p>

<p><img src="https://cdn-images-1.medium.com/max/2000/1*f2g7p9B3QAaIo8rO8v265w.png" alt="Let us take sample values as inputs to the function." /><em>Let us take sample values as inputs to the function.</em></p>

<p>The partial derivative on variable x ( ∂f/ ∂x) and variable y ( ∂f/ ∂y) are 3 and -2 respectively. This gives us an understanding that increasing the variable x by an amount ε would increase the output function by 3ε. Similarly, increasing the variable y by an amount ε would decrease the output function by 2ε. Thus, the derivative of a function on each variable tells us the sensitivity of the function with respect to that variable.</p>

<p>Drawing a parallel analogy, by computing the gradients of the loss function with respect to the weights and the inputs of the neural network, we can determine the sensitivity of the loss function with respect to these parameters. These gradients are a measure of how well the neural network is performing and how the parameters of the model are affecting the loss function. It also helps us in fine tuning the weights of the network to minimize our loss and find a model that fits our data.</p>

<h2 id="how-to-backpropagate">How to backpropagate?</h2>

<p>We’ll be using the following example throughout the rest of the article.</p>

<p><img src="https://cdn-images-1.medium.com/max/2000/1*DQ5xxpPVU44Lqddo8f7Yhw.png" alt="This example will be used throughout the rest of the article." /><em>This example will be used throughout the rest of the article.</em></p>

<p>It’s a good practice to draw computation graphs and analyse the expressions, albeit easier only for simple expressions. So, let’s draw one. We also introduce intermediary variables like x and y to make our calculations simpler.</p>

<p><img src="https://cdn-images-1.medium.com/max/2000/1*azqHvbrNsZ8AIZ7H75tbIQ.jpeg" alt="Computational graph for the example f=(a+b)(b+c) with a = -1, b = 3 and c = 4." /><em>Computational graph for the example f=(a+b)(b+c) with a = -1, b = 3 and c = 4.</em></p>

<p><strong>Every node in a computational graph can compute two things — the output of the node and the local gradient of the node without even being aware of the rest of the graph.</strong> Local gradients of a node are the derivatives of the output of the node with respect to each of the inputs.</p>

<p><img src="https://cdn-images-1.medium.com/max/2000/1*7XxBjQzyLCkWKEgJD_w9jQ.png" alt="Local gradients of the nodes in the computational graph." /><em>Local gradients of the nodes in the computational graph.</em></p>

<p>We have marked the outputs on the graph and have also calculated the local gradients of the nodes. <strong>Backpropagation is a “local” process and can be viewed as a recursive application of the chain rule.</strong> Now, we want the sensitivity of our output (loss) function w.r.t to the input variables a, b and c of the graph (i.e. ∂f/ ∂a, ∂f/ ∂b and ∂f/ ∂c). We start with the output variable and find the derivative of the output of the graph w.r.t to every variable by recursive chain rule. The derivative of the output variable w.r.t itself is one.</p>

<p><img src="https://cdn-images-1.medium.com/max/2338/1*GEpvvmhoj0yRTi_kpDS6Eg.png" alt="Backpropagating and finding the gradient of f w.r.t all the variables in the graph by the application of chain rule. Blue elements represent the outputs of the nodes whereas red element represents the gradients that were calculated during backpropagation. (Note that f = (a+b)(b+c), x= a+b and y = b+c)" /><em>Backpropagating and finding the gradient of f w.r.t all the variables in the graph by the application of chain rule. Blue elements represent the outputs of the nodes whereas red element represents the gradients that were calculated during backpropagation. (Note that f = (a+b)(b+c), x= a+b and y = b+c)</em></p>

<p>Let us take one node in the graph and get a clear picture.</p>

<p><img src="https://cdn-images-1.medium.com/max/2000/1*mudCF1PDeBlErRNOPXD24Q.jpeg" alt="A closer look of backpropagation — considering a single node." /><em>A closer look of backpropagation - considering a single node.</em></p>

<p>We computed the outputs and local gradients of every node before we started backpropagating. The graph which was unaware of the existence of other nodes when we calculated the outputs and local gradients, interacts with the other nodes and learns the derivative of its output value on the final output of the graph ( ∂f/ ∂x). Thus, <strong>we find the derivative of the output of the graph w.r.t a variable (</strong> ∂f/ ∂a<strong>) by multiplying its local gradient (</strong> ∂x/ ∂a) <strong>with the upstream gradient that we receive from the node’s output value. (</strong> ∂f/ ∂x)<strong>. This is the essence of backpropagation</strong>. If we look at variable b, we can use the multivariate chain rule to find the derivative of the output f w.r.t the variable b.</p>

<p><img src="https://cdn-images-1.medium.com/max/2000/1*wT88MRBXEztY5KrRq_PzVg.png" alt="Gradients add at branches — multivariate chain rule." /><em>Gradients add at branches — multivariate chain rule.</em></p>

<p>We can also interpret this multivariate chain rule by saying “<strong>gradients add at branches</strong>”. Thus, we have found the sensitivity of variables a, b and c on the output f by computing the derivatives through backpropagation.</p>

<p>At this point, you might have the following questions.</p>

<blockquote>
<p>“Why this roundabout method of finding gradients by backpropagation while we can compute the same gradients by differentiating in a simple, forward manner?”</p>

<p>“Oh, backpropagation is nothing but chain rule! What’s so special in that?”</p>
</blockquote>

<h2 id="why-backpropagation">Why backpropagation?</h2>

<p>Let us look at two strategies by which we can compute gradients.</p>

<h3 id="strategy-1-forward-differentiation">Strategy 1: Forward differentiation</h3>

<p>It is the usual way of finding gradients, the way that we all learnt in our high school. Let us consider the same example again. Without loss of generality, we choose variable b and find gradients upwards.</p>

<p><img src="https://cdn-images-1.medium.com/max/2268/1*r5bFLL1rKCrMuZOnZR0lcg.png" alt="Forward differentiation — the traditional way. (Note that f = (a+b)(b+c), x= a+b and y = b+c)" /><em>Forward differentiation — the traditional way. (Note that f = (a+b)(b+c), x= a+b and y = b+c)</em></p>

<p><img src="https://cdn-images-1.medium.com/max/2000/1*wLd-zTw71u7GDnsgjKwKoA.png" alt="" /></p>

<p>Thus, we have computed the derivative of f (our output) w.r.t. variable b (one of the inputs).</p>

<p>Forward differentiation determines how one of the inputs affect every node in the graph.</p>

<h2 id="strategy-2-reverse-differentiation">Strategy 2: Reverse Differentiation:</h2>

<p>We already implemented reverse differentiation when we learnt how to do backpropagation. Just to have a recap, let’s look at the graph without any chain-rule steps written on the graph.</p>

<p><img src="https://cdn-images-1.medium.com/max/2360/1*U3mVDYuvnaLhJzIFw_d5qQ.png" alt="Backpropagation or reverse differentiation. Refer the diagram under 'How to Backpropagate?' for the chain-rule steps." /><em>Backpropagation or reverse differentiation. Refer the diagram under &lsquo;How to Backpropagate?&rsquo; for the chain-rule steps.</em></p>

<p>If you notice properly, by doing reverse differentiation (or backpropagation), we have computed the derivative of f (our output or loss function) with respect to every node in the graph. <strong>Yeah, you saw that right, with respect to every single node in the graph!</strong></p>

<blockquote>
<p>Forward-mode differentiation gave us the derivative of our output with respect to one of the inputs, but reverse-mode differentiation gives us all of them.</p>
</blockquote>

<p>As we have only three variables as input to the graph, we can see a thrice speedup by performing backpropagation (reverse differentiation) instead of forward differentiation.</p>

<blockquote>
<p>Why thrice speedup?</p>
</blockquote>

<p><img src="https://cdn-images-1.medium.com/max/2000/1*9tfrBe7pAtqEiVwz5w7j3A.png" alt="" /></p>

<blockquote>
<p>We found only the derivative of f w.r.t b in forward differentiation.</p>
</blockquote>

<p><img src="https://cdn-images-1.medium.com/max/2000/1*Ua0MG2skPgLovioPghKm5g.png" alt="" /></p>

<blockquote>
<p>Whereas, we found the derivative of f w.r.t all the three input variables, by backprop in one fell swoop.</p>
</blockquote>

<h3 id="so-is-that-all">So, is that all?</h3>

<p><img src="https://cdn-images-1.medium.com/max/600/0*rBQI7uBhBKE8KT-X.png" alt="Gradient Descent. J(w) represents the loss and w represents the weight. Source: Google Images" /><em>Gradient Descent. J(w) represents the loss and w represents the weight. Source: Google Images</em></p>

<p>To reiterate, loss function quantifies the quality of our weights. Having calculated the gradients of our loss function with respect to all the parameters of the neural networks, its time to update the model parameters using these gradients to make our model more fit to the data. A commonly used technique to optimize weight parameters is <strong>gradient descent</strong>. In gradient descent, we take small baby steps in the direction of the minima to get optimized weight parameters. The size of the steps that we take to reach the optimum value is determined by a parameter called <strong>learning rate.</strong> Other commonly used techniques for weight updation are AdaGrad, RMSProp and Adam optimization. Thus, by making use of the gradients computed through an efficient backprop, we are able to find the best set of weights that minimizes our loss function. We do this by backpropagating the neural network multiple times until we reach a steady loss.</p>

<h2 id="yo-backprop-is-powerful">Yo, backprop is powerful!</h2>

<p>Convolutional Neural Networks (CNNs) are a class of deep neural networks (deep, implying large number of hidden layers) that are primarily used for visual recognition — classifying images. ImageNet, the largest visual database consists of over 14 million images belonging to 20 thousand categories. It is a common practice to evaluate the performance of a CNN by training and testing it on the ImageNet database due to the large number of labeled images that it has. The current standard CNN architecture that performs the best on ImageNet is ResNet-152, which has 152 layers and a parameter count close to a billion! By performing backpropagation, we can get the gradients of the loss function with respect to all the inputs and weights of the network. Think of the massive speedup of billion times when we choose backpropagation over forward differentiation. Sounds awesome, doesn’t it?</p>

<p>Hope you understood the actual intuition behind backpropagation and why it is preferred. Cheers! :)</p>

<h3 id="references">References:</h3>

<ol>
<li><p><a href="http://cs231n.github.io/optimization-2/" target="_blank">http://cs231n.github.io/optimization-2/</a></p></li>

<li><p><a href="https://www.youtube.com/watch?v=d14TUNcbn1k" target="_blank">https://www.youtube.com/watch?v=d14TUNcbn1k</a></p></li>

<li><p><a href="http://colah.github.io/posts/2015-08-Backprop/" target="_blank">http://colah.github.io/posts/2015-08-Backprop/</a></p></li>

<li><p><a href="https://medium.com/@karpathy/yes-you-should-understand-backprop-e2f06eab496b" target="_blank">https://medium.com/@karpathy/yes-you-should-understand-backprop-e2f06eab496b</a></p></li>
</ol>

<h3 id="more-reading">More reading:</h3>

<ol>
<li><p><a href="http://neuralnetworksanddeeplearning.com/chap2.html" target="_blank">http://neuralnetworksanddeeplearning.com/chap2.html</a></p></li>

<li><p><a href="http://cs231n.stanford.edu/handouts/derivatives.pdf" target="_blank">http://cs231n.stanford.edu/handouts/derivatives.pdf</a></p></li>
</ol>

<p>P. S. Used LaTeX and <a href="http://www.draw.io" target="_blank">www.draw.io</a> for the network diagrams.</p>

    </div>

    


<div class="article-tags">
  
  <a class="label label-default" href="http://benedictflorance.github.io/tags/machine-learning/">machine-learning</a>
  
  <a class="label label-default" href="http://benedictflorance.github.io/tags/deep-learning/">deep-learning</a>
  
  <a class="label label-default" href="http://benedictflorance.github.io/tags/ai/">ai</a>
  
</div>




    
    

    

    
<section id="comments">
  <div id="disqus_thread"></div>
<script>
    var disqus_config = function () {
    
    
    
    };
    (function() {
        if (["localhost", "127.0.0.1"].indexOf(window.location.hostname) != -1) {
            document.getElementById('disqus_thread').innerHTML = 'Disqus comments not available by default when the website is previewed locally.';
            return;
        }
        var d = document, s = d.createElement('script'); s.async = true;
        s.src = '//' + "benedictflorance" + '.disqus.com/embed.js';
        s.setAttribute('data-timestamp', +new Date());
        (d.head || d.body).appendChild(s);
    })();
</script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
<a href="https://disqus.com" class="dsq-brlink">comments powered by <span class="logo-disqus">Disqus</span></a>
</section>



  </div>
</article>

<footer class="site-footer">
  <div class="container">

    

    <p class="powered-by">

      &copy; 2018  · Benedict Florance Arockiaraj &middot; 

      Powered by the
      <a href="https://sourcethemes.com/academic/" target="_blank" rel="noopener">Academic theme</a> for
      <a href="https://gohugo.io" target="_blank" rel="noopener">Hugo</a>.

      <span class="pull-right" aria-hidden="true">
        <a href="#" id="back_to_top">
          <span class="button_icon">
            <i class="fa fa-chevron-up fa-2x"></i>
          </span>
        </a>
      </span>

    </p>
  </div>
</footer>


<div id="modal" class="modal fade" role="dialog">
  <div class="modal-dialog">
    <div class="modal-content">
      <div class="modal-header">
        <button type="button" class="close btn-large" data-dismiss="modal">&times;</button>
        <h4 class="modal-title">Cite</h4>
      </div>
      <div>
        <pre><code class="modal-body tex"></code></pre>
      </div>
      <div class="modal-footer">
        <a class="btn btn-primary btn-outline js-copy-cite" href="#" target="_blank">
          <i class="fa fa-copy"></i> Copy
        </a>
        <a class="btn btn-primary btn-outline js-download-cite" href="#" target="_blank">
          <i class="fa fa-download"></i> Download
        </a>
        <div id="modal-error"></div>
      </div>
    </div>
  </div>
</div>

    

    
    
    <script id="dsq-count-scr" src="//benedictflorance.disqus.com/count.js" async></script>
    

    
    <script async defer src="//maps.googleapis.com/maps/api/js?key=AIzaSyBwv9cYYuaYjSmnnclWcDip_WIUOkIuGoU"></script>
    

    
    <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.2.1/jquery.min.js" integrity="sha512-3P8rXCuGJdNZOnUx/03c1jOTnMn3rP63nBip5gOP2qmUh5YAdVAvFZ1E+QLZZbC1rtMrQb+mah3AfYW11RUrWA==" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery.imagesloaded/4.1.3/imagesloaded.pkgd.min.js" integrity="sha512-umsR78NN0D23AzgoZ11K7raBD+R6hqKojyBZs1w8WvYlsI+QuKRGBx3LFCwhatzBunCjDuJpDHwxD13sLMbpRA==" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/3.3.7/js/bootstrap.min.js" integrity="sha512-iztkobsvnjKfAtTNdHkGVjAYTrrtlC7mGp/54c40wowO7LhURYl3gVzzcEqGl/qKXQltJ2HwMrdLcNUdo+N/RQ==" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery.isotope/3.0.4/isotope.pkgd.min.js" integrity="sha512-VDBOIlDbuC4VWxGJNmuFRQ0Li0SKkDpmGyuhAG5LTDLd/dJ/S0WMVxriR2Y+CyPL5gzjpN4f/6iqWVBJlht0tQ==" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.2.5/jquery.fancybox.min.js" integrity="sha256-X5PoE3KU5l+JcX+w09p/wHl9AzK333C4hJ2I9S5mD4M=" crossorigin="anonymous"></script>
    
    <script src="https://cdnjs.cloudflare.com/ajax/libs/gmaps.js/0.4.25/gmaps.min.js" integrity="sha256-7vjlAeb8OaTrCXZkCNun9djzuB2owUsaO72kXaFDBJs=" crossorigin="anonymous"></script>
    
    
    <script src="/js/hugo-academic.js"></script>
    

    
    
      
      
      <script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js" integrity="sha256-/BfiIkHlHoVihZdc6TFuj7MmJ0TWcWsMXkeDFwhi0zw=" crossorigin="anonymous"></script>
      

      

      

      <script>hljs.initHighlightingOnLoad();</script>
    

    
    

  </body>
</html>

